

from typing import Optional, Tuple, Dict

import torch
import torch.nn as nn
import torch.nn.functional as F

import dgl
from dgl import DGLGraph  # íƒ€ì… íŒíŠ¸ ìš©ë„ (ì„ íƒ)

# GNN ë°±ë³¸ì€ models íŒ¨í‚¤ì§€ì—ì„œ ê°€ì ¸ì˜¤ê¸°
from models import GCN, GAT, GIN, GIN_MoE
from models.GIN_MoE import GIN_MoE
from models.GCN_MoE import GCN_MoE
from models.Graphormer import GraphormerEncoder, GraphormerEncoderMoE
from models.moe_module import ExpertModule, ExpertModule_3loss
from models.moe_fusion_module import FusionMoE

from core.config import Settings
# -----------------------------
# Predictor (ì—£ì§€ ë¶„ë¥˜ê¸°)
# -----------------------------

class MLPPredictor(nn.Module):
    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int, dropout: float = 0.2):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(2 * in_dim, hidden_dim), nn.ReLU(),
            nn.Dropout(dropout), nn.Linear(hidden_dim, out_dim),
        )
    # def forward(self, g: DGLGraph, z: torch.Tensor, eids: Optional[torch.Tensor] = None, uv=None):
    #     u, v = g.find_edges(eids) if uv is None else uv
    #     return self.mlp(torch.cat([z[u], z[v]], dim=-1))
        # fixed_nidì™€ ê¸°íƒ€ í‚¤ì›Œë“œëŠ” ë°›ì•„ì„œ ê·¸ëƒ¥ ë¬´ì‹œ
    def forward(self, g: DGLGraph, z: torch.Tensor, eids: Optional[torch.Tensor] = None, uv=None,
                fixed_nid: Optional[int] = None, **kwargs):
        u, v = g.find_edges(eids) if uv is None else uv
        return self.mlp(torch.cat([z[u], z[v]], dim=-1))
    
class EdgeMoEPredictor(nn.Module):
    """
    GAT/GINì—ì„œ ë‚˜ì˜¨ zë¥¼ ë°›ì•„ì„œ,
    ì„œë¡œ ë‹¤ë¥¸ 4ê°€ì§€ scoring expertë¥¼ mixtureë¡œ ì“°ëŠ” ë§í¬ ì˜ˆì¸¡ MoE predictor.
    return: (out, aux_loss)
    """
    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int,
                 dropout: float = 0.2, num_experts: int = 4, top_k: int=1,):
        super().__init__()
        assert num_experts == 4, "í˜„ì¬ êµ¬í˜„ì€ 4ê°œ expertì— ë§ì¶°ì ¸ ìˆìŒ"
        self.num_experts = num_experts
        self.dropout = nn.Dropout(dropout)
        self.top_k = top_k

        D = in_dim
        edge_feat_dim = 4 * D  # [z_u, z_v, |z_u-z_v|, z_u*z_v]

        # ğŸ”¹ ê²Œì´íŠ¸ ë„¤íŠ¸ì›Œí¬: edge feature â†’ expert weight
        self.gate = nn.Linear(edge_feat_dim, num_experts)

        # ğŸ”¹ Expert 1: concat ê¸°ë°˜
        self.expert_concat = nn.Sequential(
            nn.Linear(2 * D, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim),
        )

        # ğŸ”¹ Expert 2: distance ê¸°ë°˜ (|z_u - z_v|)
        self.expert_dist = nn.Sequential(
            nn.Linear(D, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim),
        )

        # ğŸ”¹ Expert 3: multiplicative ê¸°ë°˜ (z_u * z_v)
        self.expert_mul = nn.Sequential(
            nn.Linear(D, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim),
        )

        # ğŸ”¹ Expert 4: all-in-one (concat + diff + mul)
        self.expert_all = nn.Sequential(
            nn.Linear(edge_feat_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim),
        )

    def forward(self, g: DGLGraph, z: torch.Tensor,
                eids: Optional[torch.Tensor] = None, uv=None,
                fixed_nid: Optional[int] = None, **kwargs):
        # u, v ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ MLPPredictorì™€ ë™ì¼)
        u, v = g.find_edges(eids) if uv is None else uv  # (B,)

        z_u = z[u]  # (B, D)
        z_v = z[v]  # (B, D)

        z_cat  = torch.cat([z_u, z_v], dim=-1)      # (B, 2D)
        z_diff = torch.abs(z_u - z_v)               # (B, D)
        z_mul  = z_u * z_v                          # (B, D)

        edge_feat = torch.cat([z_cat, z_diff, z_mul], dim=-1)  # (B, 4D)

        # 1) ê° expertì˜ score ê³„ì‚°
        s1 = self.expert_concat(z_cat)    # (B, out_dim)
        s2 = self.expert_dist(z_diff)     # (B, out_dim)
        s3 = self.expert_mul(z_mul)       # (B, out_dim)
        s4 = self.expert_all(edge_feat)   # (B, out_dim)

        scores = torch.stack([s1, s2, s3, s4], dim=1)  # (B, 4, out_dim)

        # 2) ê²Œì´íŠ¸ë¡œ mixture weight ê³„ì‚°
        gate_logits = self.gate(edge_feat)           # (B, 4)
        gate_probs = F.softmax(gate_logits, dim=-1)  # (B, 4)
        # gate_probs_expanded = gate_probs.unsqueeze(-1)  # (B, 4, 1)

        ################ top-K routing #################
        top_k = self.top_k
        if top_k == self.num_experts:
            # ëª¨ë“  expertë¥¼ ì“°ëŠ” ê²½ìš°: ì¼ë°˜ soft mixture (aux_lossë§Œ ìœ ì§€)
            gate_st = gate_probs
        else:
            # ìƒìœ„ kê°œì˜ probì™€ index
            topk_vals, topk_idx = torch.topk(gate_probs, k=top_k, dim=-1)  # (B, k), (B, k)

            # hard_gate: ì„ íƒëœ kê°œ ìœ„ì¹˜ì—ë§Œ ê°’ì„ ë‘ê³  ë‚˜ë¨¸ì§€ëŠ” 0
            # ì—¬ê¸°ì„œëŠ” ì„ íƒëœ ìœ„ì¹˜ì— ì›ë˜ soft probë¥¼ ë‚¨ê¹€ (0/1ì´ ì•„ë‹ˆë¼ "zero-outëœ soft")
            hard_gate = torch.zeros_like(gate_probs)  # (B, num_experts)
            hard_gate.scatter_(1, topk_idx, topk_vals)

            # ğŸ”¹ Straight-through trick:
            # forward: hard_gateì²˜ëŸ¼ ë™ì‘
            # backward: gate_probsì—ì„œ gradientë¥¼ ë°›ë„ë¡ êµ¬ì„±
            gate_st = hard_gate + gate_probs - gate_probs.detach()  # (B, num_experts)

        gate_st_expanded = gate_st.unsqueeze(-1)                 # (B, num_experts, 1)
        ################ top-K routing #################

        # 3) ê°€ì¤‘í•©
        # out = (gate_probs_expanded * scores).sum(dim=1)  # (B, out_dim)

        out = (gate_st_expanded * scores).sum(dim=1)  # (B, out_dim)
        out = self.dropout(out)

        # 4) load balancing auxiliary loss
        # ê° expertê°€ ë°°ì¹˜ì—ì„œ ì–¼ë§ˆë‚˜ ì‚¬ìš©ë˜ëŠ”ì§€ í‰ê·  (soft mixture ê¸°ì¤€)
        # avg_expert_usage: (4,)
        avg_expert_usage = gate_probs.mean(dim=0)
        # usageê°€ í•œìª½ìœ¼ë¡œ ì ë¦¬ì§€ ì•Šê²Œ L2 penalty
        aux_loss = (avg_expert_usage ** 2).sum() * self.num_experts

        return out, aux_loss


class HeteroProjectionGNN(nn.Module):
    """
    ê° ë…¸ë“œ íƒ€ì…(person, disease)ì˜ í”¼ì²˜ë¥¼ íƒ€ì…ë³„ í”„ë¡œì ì…˜ ë ˆì´ì–´ë¥¼ í†µí•´
    ê³µí†µëœ hidden_dimìœ¼ë¡œ ë§ì¶˜ í›„, GNN ë°±ë³¸ì— ì „ë‹¬í•˜ëŠ” ëª¨ë¸
    """
    def __init__(self, person_in_dim: int, disease_in_dim: int, params: Dict, g_hetero: dgl.DGLHeteroGraph):
        super().__init__()
        self.model_type = params['model_type']
        hidden_dim = params['hidden_dim']
        
        # 1. íƒ€ì…ë³„ í”„ë¡œì ì…˜ ë ˆì´ì–´ë¥¼ ModuleDictë¡œ ê´€ë¦¬
        # ê° íƒ€ì…ì„ hidden_dimìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.
        self.projectors = nn.ModuleDict({
            'person': nn.Linear(person_in_dim, hidden_dim),
            'disease': nn.Linear(disease_in_dim, hidden_dim)
        })
        
        # 2. GNN ë°±ë³¸ì€ ì´ì œ hidden_dimì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
        activation, tasks, causal = F.relu, [], False
        if self.model_type == "gcn":
            self.gnn = GCN(hidden_dim, hidden_dim, hidden_dim, params['n_layers'], activation, params['dropout'], tasks, causal)
        elif self.model_type == "gat":
            self.gnn = GAT(params['n_layers'], hidden_dim, hidden_dim, hidden_dim, params['num_heads'], activation, params['dropout'], params['gat_attn_drop'], params['gat_neg_slope'], True, tasks, causal)
        elif self.model_type == "gin":
            self.gnn = GIN(hidden_dim, hidden_dim, hidden_dim, params['n_layers'], params['gin_mlp_layers'], params['dropout'], tasks, causal, "mean", True)
        elif self.model_type == "gin_moe":
            self.gnn = GIN_MoE(hidden_dim, hidden_dim, hidden_dim, params['n_layers'], params['gin_mlp_layers'], params['dropout'], tasks, causal, "mean", True)
        elif self.model_type == "gcn_moe":
            self.gnn = GCN_MoE(hidden_dim, hidden_dim, hidden_dim, params['n_layers'], activation, params['dropout'], tasks, causal)

        elif self.model_type in ["multi_graph","multi_graph_pred_moe"]:
            # self.gcn = GCN(hidden_dim, hidden_dim, hidden_dim, params['n_layers'], activation, params['dropout'], tasks, causal)
            # self.gat = GAT(params['n_layers'], hidden_dim, hidden_dim, hidden_dim, params['num_heads'], activation, params['dropout'], params['gat_attn_drop'], params['gat_neg_slope'], True, tasks, causal)
            self.gin = GIN(hidden_dim, hidden_dim, hidden_dim, params['n_layers'], params['gin_mlp_layers'], params['dropout'], tasks, causal, "mean", True)
        else:
            raise ValueError(f"Unknown model_type for HeteroProjectionGNN: {self.model_type}")
            
        # 3. ë…¸ë“œ íƒ€ì… ì´ë¦„ê³¼ DGLì´ ë¶€ì—¬í•œ ì •ìˆ˜ IDë¥¼ ë§¤í•‘
        self.ntype_map = {ntype: i for i, ntype in enumerate(g_hetero.ntypes)}
        self.in_dim = hidden_dim # GNNì˜ ì…ë ¥ ì°¨ì›ì€ hidden_dim
        # self.reduce_384To128 = nn.Linear(128*3, 128)
        # self.alpha = torch.nn.Parameter(torch.tensor(0.5))  # ì´ˆê¸°ê°’ 0.5, í•™ìŠµ ê°€ëŠ¥

    def forward(self, g: dgl.DGLGraph, features: torch.Tensor) -> torch.Tensor:
        # ìµœì¢… GNNì— ì…ë ¥ë , í”„ë¡œì ì…˜ì´ ì™„ë£Œëœ í”¼ì²˜ í…ì„œ
        projected_feats = torch.zeros(g.num_nodes(), self.in_dim, device=g.device)

        # 2. íƒ€ì…ë³„ë¡œ ìˆœíšŒí•˜ë©° ê°ê°ì˜ í”„ë¡œì í„°ë¥¼ ì ìš©
        for ntype, projector in self.projectors.items():
            type_id = self.ntype_map[ntype]
            mask = (g.ndata['_TYPE'] == type_id)
            
            # ì›ë³¸ í”¼ì²˜ ìŠ¬ë¼ì´ì‹± (ì œë¡œ íŒ¨ë”© ë¶€ë¶„ ì œê±°)
            original_dim = projector.in_features
            original_h = features[mask, :original_dim]
            
            # í•´ë‹¹ íƒ€ì…ì˜ í”„ë¡œì í„° í†µê³¼
            projected_h = projector(original_h)
            
            # ê²°ê³¼ í…ì„œì— ì €ì¥
            projected_feats[mask] = projected_h
            
        # 3. GNN ë°±ë³¸ì— í†µê³¼ì‹œì¼œ ìµœì¢… ë…¸ë“œ ì„ë² ë”©(z)ì„ ì–»ìŒ
        if self.model_type in ["multi_graph","multi_graph_pred_moe"]:
            # z_gcn = self.gcn(g, projected_feats)
            # z_gat = self.gat(g, projected_feats)
            z = self.gin(g, projected_feats)
            # z = torch.cat([z_gat, z_gin], dim=-1)
            # z = self.reduce_384To128(z_cat)
            # z = self.alpha * z_gcn + (1-self.alpha) * z_gat
        else:
            z = self.gnn(g, projected_feats)
        return z
    
    # def forward_edges(self, g: dgl.DGLGraph, features: torch.Tensor, predictor: nn.Module, uv: Tuple) -> torch.Tensor:
    #     """ í•™ìŠµ ë£¨í”„ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ì—£ì§€ í¬ì›Œë“œ ë©”ì†Œë“œ """
    #     z = self.forward(g, features)
    #     return predictor(g, z, uv=uv)
    
    def forward_edges(self, g: dgl.DGLGraph, features: torch.Tensor, predictor: nn.Module, uv: Tuple) -> torch.Tensor:
        """ í•™ìŠµ ë£¨í”„ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ì—£ì§€ í¬ì›Œë“œ ë©”ì†Œë“œ """
        z = self.forward(g, features)
        # return predictor(g, z[0], uv=uv), z[1]        # moe ì“¸ ë•Œ
        return predictor(g, z, uv=uv)


class HeteroProjectionMoEGNN(nn.Module):
    """
    ê° ë…¸ë“œ íƒ€ì…(person, disease)ì˜ í”¼ì²˜ë¥¼ íƒ€ì…ë³„ í”„ë¡œì ì…˜ ë ˆì´ì–´ë¥¼ í†µí•´
    ê³µí†µëœ hidden_dimìœ¼ë¡œ ë§ì¶˜ í›„, GNN + MoE ê¸°ë°˜ ë°±ë³¸ì„ í†µí•´
    ë…¸ë“œ ì„ë² ë”©(z)ì„ ê³„ì‚°í•˜ëŠ” ëª¨ë¸.
    """
    def __init__(self, person_in_dim: int, disease_in_dim: int, params: dict, g_hetero: dgl.DGLHeteroGraph):
        super().__init__()
        self.model_type = params['model_type']
        hidden_dim = params['hidden_dim']
        activation, tasks, causal = F.relu, [], False

        # 1ï¸âƒ£ íƒ€ì…ë³„ projector
        self.projectors = nn.ModuleDict({
            'person': nn.Linear(person_in_dim, hidden_dim),
            'disease': nn.Linear(disease_in_dim, hidden_dim)
        })

        # 2ï¸âƒ£ GNN ë°±ë³¸
        if self.model_type == "multi_graph":
            self.gcn = GCN(hidden_dim, hidden_dim, hidden_dim,
                           params['n_layers'], activation,
                           params['dropout'], tasks, causal)
            self.gat = GAT(params['n_layers'], hidden_dim, hidden_dim, hidden_dim,
                           params['num_heads'], activation, params['dropout'],
                           params['gat_attn_drop'], params['gat_neg_slope'],
                           True, tasks, causal)
            self.gin = GIN(hidden_dim, hidden_dim, hidden_dim, params['n_layers'], params['gin_mlp_layers'], params['dropout'], tasks, causal, "mean", True)
        else:
            raise ValueError(f"Unknown model_type for HeteroProjectionGNN: {self.model_type}")

        # 3ï¸âƒ£ Expert Modules (Top-1, Expert 4ê°œ)
        self.k = params.get("topk", 1)
        self.expert_gcn = ExpertModule(hidden_dim, hidden_dim // 2,
                                       num_experts=4, k=self.k)
        self.expert_gat = ExpertModule(hidden_dim, hidden_dim // 2,
                                       num_experts=4, k=self.k)
        self.expert_gin = ExpertModule(hidden_dim, hidden_dim // 2,
                                       num_experts=4, k=self.k)

        # íƒ€ì… ë§¤í•‘
        self.ntype_map = {ntype: i for i, ntype in enumerate(g_hetero.ntypes)}
        self.in_dim = hidden_dim
        
        # ğŸ” Gated multiplicative fusion ëª¨ë“ˆ
        # u = [z_gat; z_gin] (N, 2D) -> gate g (N, D)
        self.fusion_gate = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Sigmoid()
        )
        # concat ìª½ì—ì„œ ì˜¤ëŠ” additive term u' ìƒì„±ìš©
        self.fusion_linear = nn.Linear(hidden_dim * 2, hidden_dim)

    # -----------------------------
    # ğŸ§  projector ì ìš©
    # -----------------------------
    def _project_and_embed(self, g, features):
        projected_feats = torch.zeros(g.num_nodes(), self.in_dim, device=g.device)
        for ntype, projector in self.projectors.items():
            type_id = self.ntype_map[ntype]
            mask = (g.ndata['_TYPE'] == type_id)
            original_dim = projector.in_features
            projected_feats[mask] = projector(features[mask, :original_dim])
        return projected_feats

    # -----------------------------
    # âš™ï¸ forward(): GNN + MoE
    # -----------------------------
    def forward(self, g: dgl.DGLGraph, features: torch.Tensor):
        """
        GNN ì„ë² ë”©(z)ì„ ê³„ì‚°í•˜ê³ , MoE layer í†µê³¼ í›„
        element-wise multiplicationìœ¼ë¡œ ìµœì¢… zë¥¼ ë°˜í™˜.
        Load balancing loss í¬í•¨.
        """
        projected_feats = self._project_and_embed(g, features)

        # 1ï¸âƒ£ GNN ê¸°ë°˜ ì„ë² ë”© ê³„ì‚°
        # z_gcn = self.gcn(g, projected_feats)
        z_gat = self.gat(g, projected_feats)
        z_gin = self.gin(g, projected_feats)

        # 2ï¸âƒ£ Expert MoE í†µê³¼
        # z_gcn_expert, aux_gcn = self.expert_gcn(z_gcn)
        z_gat_expert, aux_gat = self.expert_gat(z_gat)
        z_gin_expert, aux_gin = self.expert_gin(z_gin)

        # 3ï¸âƒ£ Element-wise multiplication
        z = z_gin_expert * z_gat_expert


        ###################### Gated mul fusion ######################
        u = torch.cat([z_gat_expert, z_gin_expert], dim=-1)
        g = self.fusion_gate(u)

        u_prime = self.fusion_gate(u)
        z = g*z + (1.0 - g) * u_prime
        ###################### Gated mul fusion ######################


        # 4ï¸âƒ£ load balancing loss í•©ì‚°
        aux_loss_total = aux_gin + aux_gat
        # aux_loss_total = aux_gat

        # 5ï¸âƒ£ forward_edges()ì™€ í˜¸í™˜ì„± ìœ ì§€ ìœ„í•´ íŠœí”Œ ë°˜í™˜
        # z = z_gat_expert
        return z, aux_loss_total

    # -----------------------------
    # ğŸ”— forward_edges(): ê¸°ì¡´ êµ¬ì¡° ìœ ì§€
    # -----------------------------
    def forward_edges(self, g: dgl.DGLGraph, features: torch.Tensor, predictor: nn.Module, uv: tuple):
        """
        í•™ìŠµ ë£¨í”„ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ì—£ì§€ í¬ì›Œë“œ ë©”ì†Œë“œ.
        forward()ì—ì„œ z, aux_loss ê³„ì‚° í›„ predictor í˜¸ì¶œ.
        """
        z, aux_loss = self.forward(g, features)
        pred = predictor(g, z, uv=uv)
        return pred, aux_loss




class HeteroProjectionGraphormer(nn.Module):
    """
    ë‘ íƒ€ì…(person, disease) í”¼ì²˜ë¥¼ ê°ê° í”„ë¡œì ì…˜ í›„ Graphormer ë°±ë³¸ì— ì „ë‹¬
    """
    def __init__(self, person_in_dim, disease_in_dim, params, g_hetero):
        super().__init__()
        hidden_dim = params['hidden_dim']
        self.in_dim = hidden_dim
        self.model_type = params['model_type']

        # íƒ€ì…ë³„ feature projection
        self.projectors = nn.ModuleDict({
            'person': nn.Linear(person_in_dim, hidden_dim),
            'disease': nn.Linear(disease_in_dim, hidden_dim)
        })

        # Graphormer backbone
        # self.encoder = GraphormerEncoder(
        #     hidden_dim=hidden_dim,
        #     num_heads=params.get('num_heads', 4),
        #     num_layers=params.get('n_layers', 2),
        #     dropout=params.get('dropout', 0.1)
        # )

        # Graphormer MoE backbone
        self.encoder = GraphormerEncoderMoE(
            hidden_dim=hidden_dim,
            num_heads=params.get('num_heads', 4),
            num_layers=params.get('n_layers', 1),
            dropout=params.get('dropout', 0.1)
        )

        # ë…¸ë“œ íƒ€ì… ë§¤í•‘
        self.ntype_map = {ntype: i for i, ntype in enumerate(g_hetero.ntypes)}

    def forward(self, g, features):
        """
        g: DGLHeteroGraph (converted to homogeneous inside)
        features: [N, total_dim]
        """
        projected_feats = torch.zeros(g.num_nodes(), self.in_dim, device=g.device)

        # íƒ€ì…ë³„ projection
        for ntype, projector in self.projectors.items():
            type_id = self.ntype_map[ntype]
            mask = (g.ndata['_TYPE'] == type_id)
            original_dim = projector.in_features
            projected_feats[mask] = projector(features[mask, :original_dim])

        # DGL â†’ edge_index ë³€í™˜
        src, dst = g.edges()
        edge_index = torch.stack([src, dst], dim=0).to(features.device)

        # Graphormer ì¸ì½”ë”©
        # z = self.encoder(projected_feats, edge_index)
        # return z
        # GraphormerMoE ì¸ì½”ë”©
        z, aux_loss = self.encoder(projected_feats, edge_index)
        return z, aux_loss

    # def forward_edges(self, g, features, predictor, uv):
    #     z = self.forward(g, features)
    #     logits = predictor(g, z, uv=uv)
    #     return logits
    
    def forward_edges(self, g, features, predictor, uv):
        z, aux_loss = self.forward(g, features)
        logits = predictor(g, z, uv=uv)
        return logits, aux_loss


class HeteroProjectionMoEFusion(nn.Module):
    def __init__(self, person_in_dim: int, disease_in_dim: int,
                 params: dict, g_hetero: dgl.DGLHeteroGraph):
        super().__init__()
        self.model_type = params['model_type']
        hidden_dim = params['hidden_dim']
        activation, tasks, causal = F.relu, [], False

        # 1ï¸âƒ£ íƒ€ì…ë³„ projector
        self.projectors = nn.ModuleDict({
            'person': nn.Linear(person_in_dim, hidden_dim),
            'disease': nn.Linear(disease_in_dim, hidden_dim)
        })

        # 2ï¸âƒ£ GNN ë°±ë³¸
        if self.model_type == "multi_graph_moe_fuse":
            self.gat = GAT(params['n_layers'], hidden_dim, hidden_dim, hidden_dim,
                           params['num_heads'], activation, params['dropout'],
                           params['gat_attn_drop'], params['gat_neg_slope'],
                           True, tasks, causal)
            self.gin = GIN(hidden_dim, hidden_dim, hidden_dim, params['n_layers'],
                           params['gin_mlp_layers'], params['dropout'],
                           tasks, causal, "mean", True)
        else:
            raise ValueError(f"Unknown model_type for HeteroProjectionGNN: {self.model_type}")

        # 3ï¸âƒ£ Fusion MoE
        self.k = params.get("topk", 1)
        self.fusion_moe = FusionMoE(
            dim=hidden_dim,
            num_experts=4,
            k=self.k,
            dropout=params.get("moe_dropout", 0.1),
            num_heads=params.get("fusion_num_heads", 4),
        )

        # íƒ€ì… ë§¤í•‘
        self.ntype_map = {ntype: i for i, ntype in enumerate(g_hetero.ntypes)}
        self.in_dim = hidden_dim

    def _project_and_embed(self, g, features):
        projected_feats = torch.zeros(g.num_nodes(), self.in_dim, device=g.device)
        for ntype, projector in self.projectors.items():
            type_id = self.ntype_map[ntype]
            mask = (g.ndata['_TYPE'] == type_id)
            original_dim = projector.in_features
            projected_feats[mask] = projector(features[mask, :original_dim])
        return projected_feats

    def forward(self, g: dgl.DGLGraph, features: torch.Tensor):
        projected_feats = self._project_and_embed(g, features)

        # GNN ì„ë² ë”©
        z_gat = self.gat(g, projected_feats)   # (N, D)
        z_gin = self.gin(g, projected_feats)   # (N, D)

        # Fusion MoE
        z, aux_loss = self.fusion_moe(z_gat, z_gin)  # (N, D), scalar

        return z, aux_loss

    def forward_edges(self, g: dgl.DGLGraph, features: torch.Tensor,
                      predictor: nn.Module, uv: tuple):
        z, aux_loss = self.forward(g, features)
        pred = predictor(g, z, uv=uv)
        return pred, aux_loss


# -----------------------------
# ëª¨ë¸ ìƒì„±
# -----------------------------

def create_model_and_predictor(params: Dict, settings: Settings,
                               person_in_dim: int, disease_in_dim: int,
                               g_hetero: dgl.DGLHeteroGraph):
    
    # HGTëŠ” ì´ ì„¤ê³„ì™€ ë§ì§€ ì•Šìœ¼ë¯€ë¡œ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬
    if params['model_type'] == "hgt":
        raise ValueError(f"The HeteroProjectionGNN is not configured for HGT. Please use gcn, gat, or gin.")

    # 1. ë˜í¼ ëª¨ë¸ì„ ë°”ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    if params['model_type'] in ["gcn", "gat", "gin", "multi_graph_pred_moe"]:
        model = HeteroProjectionGNN(person_in_dim, disease_in_dim, params, g_hetero)
    elif params['model_type'] in ["multi_graph", "gcn_moe", 'gat_moe','gin_moe'] and params["using_moe"]:
        model = HeteroProjectionMoEGNN(person_in_dim, disease_in_dim, params, g_hetero)
    elif params['model_type'] in ["multi_graph"] and not params["using_moe"]:
        model = HeteroProjectionGNN(person_in_dim, disease_in_dim, params, g_hetero)
    elif params['model_type'] in ["graphormer", "graphormer_moe"]:
        model = HeteroProjectionGraphormer(person_in_dim, disease_in_dim, params, g_hetero)
    elif params['model_type'] in ["multi_graph_moe_fuse"]:
        model = HeteroProjectionMoEFusion(person_in_dim, disease_in_dim, params, g_hetero)
    

    # 2. Predictorë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    if params['model_type'] in ["multi_graph_pred_moe"]:
        predictor = EdgeMoEPredictor(params['hidden_dim'], params['pred_hidden']*2, 1, params['pred_dropout'], 4, top_k=params['top_k'])
    else:
        predictor = MLPPredictor(params['hidden_dim'], params['pred_hidden'], 1, params['pred_dropout'])
    return model, predictor